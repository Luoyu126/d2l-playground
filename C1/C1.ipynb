{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f1f45ce",
   "metadata": {},
   "source": [
    "# 数据操作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87071da6",
   "metadata": {},
   "source": [
    "### 1. 存储数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a936319f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.arange(12)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2747702",
   "metadata": {},
   "source": [
    "张量沿每个轴的长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f1ffabd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af835c51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8939efad",
   "metadata": {},
   "source": [
    "reshape是通过原本的x创建一个新的张量，所以需要用一个变量来承接\n",
    "注意：ipynb只会把最后一行的值自动输出\n",
    "\n",
    "感觉理解维度的时候可以从外往里理解，比如（2，3，4）从外往里看，最外层是2个，每个2个里面是3个，每个3个里面是4个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29bdda92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5456, -0.4840, -0.2992,  0.4541],\n",
       "         [-0.7687, -0.3802, -0.0692,  2.3331],\n",
       "         [-0.0797, -1.9784,  0.6017,  0.8344]],\n",
       "\n",
       "        [[-2.7375, -0.2555, -0.4892,  1.7066],\n",
       "         [ 1.1858, -0.4356, -2.4021,  1.0778],\n",
       "         [-0.2860, -1.4387, -0.1290, -0.5272]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = x.reshape(3, 4)\n",
    "print(X)\n",
    "Y = x.reshape(-1, 4)  # -1表示自动计算\n",
    "print(Y)\n",
    "torch.zeros((2, 3, 4))\n",
    "torch.ones((2, 3, 4))\n",
    "torch.randn((2, 3, 4))  # 正态分布, 均值0，标准差1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daeaac21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 1, 4, 3],\n",
       "        [1, 2, 3, 4],\n",
       "        [4, 3, 2, 1]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a28d87",
   "metadata": {},
   "source": [
    "### 2. 运算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62da0ee",
   "metadata": {},
   "source": [
    "##### 2.1 按元素运算（形状相同的张量）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34950746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 3.,  4.,  6., 10.]),\n",
       " tensor([-1.,  0.,  2.,  6.]),\n",
       " tensor([ 2.,  4.,  8., 16.]),\n",
       " tensor([0.5000, 1.0000, 2.0000, 4.0000]),\n",
       " tensor([ 1.,  4., 16., 64.]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2, 4, 8])\n",
    "y = torch.tensor([2, 2, 2, 2])\n",
    "x + y, x - y, x * y, x / y, x**y  # **运算符是求幂运\n",
    "# 直接运算就是按元素运算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d09ee3e",
   "metadata": {},
   "source": [
    "##### 2.2 线性代数运算\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "718d1614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(12)\n",
    "len(x)  # 向量长度的\n",
    "x.shape  # shape是打印张量的每一维的长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1d36ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]]),\n",
       " tensor([[ 0,  4,  8],\n",
       "         [ 1,  5,  9],\n",
       "         [ 2,  6, 10],\n",
       "         [ 3,  7, 11]]),\n",
       " tensor([[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = x.reshape(3, 4)\n",
    "Y = X.T\n",
    "Z = Y.T\n",
    "X, Y, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d334e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " tensor([[ 0.,  2.,  4.,  6.],\n",
       "         [ 8., 10., 12., 14.],\n",
       "         [16., 18., 20., 22.],\n",
       "         [24., 26., 28., 30.],\n",
       "         [32., 34., 36., 38.]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
    "B = A.clone()  # 通过分配新内存，将A的一个副本分配给B，类似于c++中的拷贝构造函数\n",
    "A, A + B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9225a1fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.,   1.,   4.,   9.],\n",
       "        [ 16.,  25.,  36.,  49.],\n",
       "        [ 64.,  81., 100., 121.],\n",
       "        [144., 169., 196., 225.],\n",
       "        [256., 289., 324., 361.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A * B  # 按元素乘法，不是一般的矩阵乘法的定义，叫做Hadamard积"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796a436c",
   "metadata": {},
   "source": [
    "##### 2.3 张量拼接"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6423e33c",
   "metadata": {},
   "source": [
    "这里按0轴拼接，就是在0维上进行直接的拼接；按1轴拼接，就是在1维上进行直接的拼接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d38aba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [ 2.,  1.,  4.,  3.],\n",
       "         [ 1.,  2.,  3.,  4.],\n",
       "         [ 4.,  3.,  2.,  1.]]),\n",
       " tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
       "         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(12, dtype=torch.float32).reshape((3, 4))\n",
    "Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468f0172",
   "metadata": {},
   "source": [
    "##### 2.4 降维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ba98b19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12., 15., 18., 21.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.sum()  # 相当于把所有的维度都压缩掉了\n",
    "X.sum(\n",
    "    axis=0\n",
    ")  # axis=0表示按列求和,把行这个轴的维度压缩掉，每一列本来是一个向量，变成一个标量、\n",
    "# 也就是说axis表示被压缩掉的轴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df87cb12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(9.5000), tensor(9.5000))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.mean(), A.sum() / A.numel()  # numel()是元素个数\n",
    "# A.mean(axis=0), A.sum(axis=0) / A.shape[0] # axis=0表示按列求平均，压缩掉行这个轴\n",
    "# 也就是说求和，求平均等等原本是把张量变成标量的操作都可以指定降维，只降某些轴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "082a0d8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.],\n",
       "        [22.],\n",
       "        [38.],\n",
       "        [54.],\n",
       "        [70.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_A = A.sum(axis=1, keepdims=True)\n",
    "sum_A  # 这里其实是一个不降维的操作，为了后面广播的需要\n",
    "# 也就是说如果不写keepdims的话变成一个向量\n",
    "# 写了的话向量中每一个元素仍然是一个单元素的向量\n",
    "# 在广播机制中，单元素的向量和标量是不同的，如果是标量的话会报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4934c2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.1667, 0.3333, 0.5000],\n",
       "        [0.1818, 0.2273, 0.2727, 0.3182],\n",
       "        [0.2105, 0.2368, 0.2632, 0.2895],\n",
       "        [0.2222, 0.2407, 0.2593, 0.2778],\n",
       "        [0.2286, 0.2429, 0.2571, 0.2714]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A / sum_A\n",
    "# 这里相当于算了一些每一行每一个元素占这一行的比例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "279aec9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  6.,  8., 10.],\n",
       "        [12., 15., 18., 21.],\n",
       "        [24., 28., 32., 36.],\n",
       "        [40., 45., 50., 55.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.cumsum(axis=0)\n",
    "# 给定一个张量 / 数组，它会沿着指定的轴依次把前面的元素加起来，生成同样形状的新张量，每个位置保存的是“到此为止的和”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f02cfc",
   "metadata": {},
   "source": [
    "向量点积调用torch.dot()\n",
    "矩阵向量乘法调用torch.mv()\n",
    "矩阵乘法调用torch.mm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0789d0b",
   "metadata": {},
   "source": [
    "### 3. 广播机制"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8468848",
   "metadata": {},
   "source": [
    "对于形状不相同的张量，广播机制会自动扩展形状较小的张量，使其与形状较大的张量具有相同的形状，从而可以进行按元素运算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0fcc58",
   "metadata": {},
   "source": [
    "### 4. 索引和切片"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5fc034",
   "metadata": {},
   "source": [
    "1. 索引和切片类似数组，左闭右开，索引的时候按照先后顺序分别表示0，1，……维； 在某一个维度下只写一个冒号冒号表示取该维度的所有元素"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e88191",
   "metadata": {},
   "source": [
    "### 5. 原地操作 异地操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c3174c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2077409693760\n",
      "2077409693760\n",
      "2077409693760\n",
      "2077433066496\n"
     ]
    }
   ],
   "source": [
    "X = torch.arange(12).reshape((3, 4))\n",
    "Y = torch.zeros_like(X)\n",
    "print(id(Y))\n",
    "Y[:] = X + 1  # 原地操作\n",
    "print(id(Y))\n",
    "Y += X + 1  # 原地操作\n",
    "print(id(Y))\n",
    "Y = X + 1  # 异地操作\n",
    "print(id(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08256765",
   "metadata": {},
   "source": [
    "### 6. 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "009d6072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(os.path.join(\"..\", \"data\"), exist_ok=True)\n",
    "data_file = os.path.join(\"..\", \"data\", \"house_tiny.csv\")\n",
    "with open(data_file, \"w\") as f:\n",
    "    f.write(\"NumRooms,Alley,Price\\n\")  # 列名\n",
    "    f.write(\"NA,Pave,127500\\n\")  # 每行表示一个数据样本\n",
    "    f.write(\"2,NA,106000\\n\")\n",
    "    f.write(\"4,NA,178100\\n\")\n",
    "    f.write(\"NA,NA,140000\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86e4741b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms Alley   Price\n",
      "0       NaN  Pave  127500\n",
      "1       2.0   NaN  106000\n",
      "2       4.0   NaN  178100\n",
      "3       NaN   NaN  140000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(data_file)  # 这里datafile是一个字符串，表示文件路径\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2975837f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms Alley\n",
      "0       3.0  Pave\n",
      "1       2.0   NaN\n",
      "2       4.0   NaN\n",
      "3       3.0   NaN\n"
     ]
    }
   ],
   "source": [
    "inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]\n",
    "inputs = inputs.fillna(inputs.mean())  # 只对数值型特征做均值填充\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76ec010d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms  Alley_Pave  Alley_nan\n",
      "0       3.0           1          0\n",
      "1       2.0           0          1\n",
      "2       4.0           0          1\n",
      "3       3.0           0          1\n"
     ]
    }
   ],
   "source": [
    "inputs = pd.get_dummies(inputs, dummy_na=True)\n",
    "# 这里dummies是处理类别类型时常用的一个函数，可以把类别变量变成哑变量\n",
    "# dummy_na=True表示把缺失值也当作一个类别，即生成*_nan列\n",
    "# 生成的哑变量列名是根据原始列名加上类别值命名的，可以有多个类别\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69bb601",
   "metadata": {},
   "source": [
    "下面的函数中，to_numpy()函数把pandas的DataFrame类型转换成numpy的ndarray类型，然后再根据numpy来创建torch需要的tensor类型；这里是因为tensor只能接受numpy的ndarray类型，python的列表。注意，其实DataFrame底层逻辑也是用numpy存的，但是DataFrame接口还是和numpy不同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cfb800fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[3., 1., 0.],\n",
       "         [2., 0., 1.],\n",
       "         [4., 0., 1.],\n",
       "         [3., 0., 1.]], dtype=torch.float64),\n",
       " tensor([127500., 106000., 178100., 140000.], dtype=torch.float64))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "X = torch.tensor(inputs.to_numpy(dtype=float))\n",
    "Y = torch.tensor(outputs.to_numpy(dtype=float))\n",
    "X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1d79b0",
   "metadata": {},
   "source": [
    "### 7. 可视化处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b7c8c9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib_inline import backend_inline\n",
    "\n",
    "\n",
    "def use_svg_display():  # @save\n",
    "    \"\"\"使用svg格式在Jupyter中显示绘图\"\"\"\n",
    "    backend_inline.set_matplotlib_formats(\"svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ab1a13",
   "metadata": {},
   "source": [
    "注释#@save是一个特殊的标记，会将对应的函数、类或语句保存在d2l包中。因此，以后无须重新定义\n",
    "就可以直接调用它们（例如，d2l.use_svg_display()）\n",
    "\n",
    "无人在意，这只是d2l自己包里的一个性质"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a2cf9b",
   "metadata": {},
   "source": [
    "### 8. 自动微积分"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a92e16d",
   "metadata": {},
   "source": [
    "##### 8.1 自动微分"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70657f1c",
   "metadata": {},
   "source": [
    "一个标量函数对于一个向量的梯度是一个梯度，表示分别对每一个位置求偏导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d2645b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.arange(4.0)\n",
    "x.requires_grad_(True)  # 等价于x = torch.arange(4.0, requires_grad=True)\n",
    "print(x.grad)  # 这里grad是x的一个属性，表示x的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5827e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4., 28., 52., 76.])\n",
      "tensor([ 5., 29., 53., 77.])\n"
     ]
    }
   ],
   "source": [
    "y = 2 * torch.dot(x, x)\n",
    "y.backward()  # 这里由于y是由x得到的，所以计算backward时会自动计算x的梯度\n",
    "print(x.grad)  # 这里x.grad是一个向量，表示y对x的梯度，即偏导数\n",
    "z = x.sum()\n",
    "z.backward()\n",
    "print(x.grad)\n",
    "# .grad 只是一个普通的张量属性，每次反向传播都会把新的梯度累加进去（默认行为是 accumulate，而不是 overwrite）\n",
    "# 甚至每运算一次都会累加一次梯度！\n",
    "# 这里实际的应用是对于不同的样本进行梯度累加，所以在batch中每个样本的梯度都会累加到x.grad中"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b674319",
   "metadata": {},
   "source": [
    "深度学习框架可以自动计算导数：我们首先将梯度附加到想要对其计算偏导数的变量上，然后记录目\n",
    "标值的计算，执行它的反向传播函数，并访问得到的梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a320d116",
   "metadata": {},
   "source": [
    "### 9. 查询文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88a311c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AbsTransform', 'AffineTransform', 'Bernoulli', 'Beta', 'Binomial', 'CatTransform', 'Categorical', 'Cauchy', 'Chi2', 'ComposeTransform', 'ContinuousBernoulli', 'CorrCholeskyTransform', 'CumulativeDistributionTransform', 'Dirichlet', 'Distribution', 'ExpTransform', 'Exponential', 'ExponentialFamily', 'FisherSnedecor', 'Gamma', 'Geometric', 'Gumbel', 'HalfCauchy', 'HalfNormal', 'Independent', 'IndependentTransform', 'Kumaraswamy', 'LKJCholesky', 'Laplace', 'LogNormal', 'LogisticNormal', 'LowRankMultivariateNormal', 'LowerCholeskyTransform', 'MixtureSameFamily', 'Multinomial', 'MultivariateNormal', 'NegativeBinomial', 'Normal', 'OneHotCategorical', 'OneHotCategoricalStraightThrough', 'Pareto', 'Poisson', 'PowerTransform', 'RelaxedBernoulli', 'RelaxedOneHotCategorical', 'ReshapeTransform', 'SigmoidTransform', 'SoftmaxTransform', 'SoftplusTransform', 'StackTransform', 'StickBreakingTransform', 'StudentT', 'TanhTransform', 'Transform', 'TransformedDistribution', 'Uniform', 'VonMises', 'Weibull', 'Wishart', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'bernoulli', 'beta', 'biject_to', 'binomial', 'categorical', 'cauchy', 'chi2', 'constraint_registry', 'constraints', 'continuous_bernoulli', 'dirichlet', 'distribution', 'exp_family', 'exponential', 'fishersnedecor', 'gamma', 'geometric', 'gumbel', 'half_cauchy', 'half_normal', 'identity_transform', 'independent', 'kl', 'kl_divergence', 'kumaraswamy', 'laplace', 'lkj_cholesky', 'log_normal', 'logistic_normal', 'lowrank_multivariate_normal', 'mixture_same_family', 'multinomial', 'multivariate_normal', 'negative_binomial', 'normal', 'one_hot_categorical', 'pareto', 'poisson', 'register_kl', 'relaxed_bernoulli', 'relaxed_categorical', 'studentT', 'transform_to', 'transformed_distribution', 'transforms', 'uniform', 'utils', 'von_mises', 'weibull', 'wishart']\n",
      "Help on built-in function ones in module torch:\n",
      "\n",
      "ones(...)\n",
      "    ones(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor\n",
      "    \n",
      "    Returns a tensor filled with the scalar value `1`, with the shape defined\n",
      "    by the variable argument :attr:`size`.\n",
      "    \n",
      "    Args:\n",
      "        size (int...): a sequence of integers defining the shape of the output tensor.\n",
      "            Can be a variable number of arguments or a collection like a list or tuple.\n",
      "    \n",
      "    Keyword arguments:\n",
      "        out (Tensor, optional): the output tensor.\n",
      "        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
      "            Default: if ``None``, uses a global default (see :func:`torch.set_default_tensor_type`).\n",
      "        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
      "            Default: ``torch.strided``.\n",
      "        device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
      "            Default: if ``None``, uses the current device for the default tensor type\n",
      "            (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU\n",
      "            for CPU tensor types and the current CUDA device for CUDA tensor types.\n",
      "        requires_grad (bool, optional): If autograd should record operations on the\n",
      "            returned tensor. Default: ``False``.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> torch.ones(2, 3)\n",
      "        tensor([[ 1.,  1.,  1.],\n",
      "                [ 1.,  1.,  1.]])\n",
      "    \n",
      "        >>> torch.ones(5)\n",
      "        tensor([ 1.,  1.,  1.,  1.,  1.])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(dir(torch.distributions))  # 列出torch.distributions模块中的所有属性和方法\n",
    "help(torch.ones)  # 查看torch.ones的文档，包括具体使用方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7cc408",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
